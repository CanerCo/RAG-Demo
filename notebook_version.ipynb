{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e86307d",
   "metadata": {},
   "source": [
    "## Day 1 - Introduction to RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b2d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Environment - Day 1 Task 1\n",
    "%pip install transformers sentence-transformers  faiss-cpu gradio streamlit chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dfcdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a Dataset - Day 1 Task 2 - 3 - 4\n",
    "## Dataset: ./data/cat-facts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7be255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Environment - Day 1 Task 5\n",
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "print(generator(\"Hello, my name is\", max_length=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58667a8",
   "metadata": {},
   "source": [
    "## Day 2 - Core Components of a RAG Pipeline (Data, Embeddings, and Retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a99da3",
   "metadata": {},
   "source": [
    "### Chunking implementation (On Cat Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb70b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking implementation - Day 2 Task 1\n",
    "def chunk_text(text, max_length=500):\n",
    "    # Text is splitted into chunks at most max_length characters, at sentence boundaries if possible\n",
    "    import re\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip()) # split on sentence end\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) + 1  <= max_length:\n",
    "            current_chunk += sentence + \" \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \" \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cdb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/cat-facts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# First Chunk\n",
    "print(chunk_text(text=text, max_length=500)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d3f415",
   "metadata": {},
   "source": [
    "### Embedding the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04186f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding the Chunks - Day 2  Task 2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "chunks = chunk_text(text, max_length=500)\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "chunk_embeddings = embedding_model.encode(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c6d0d",
   "metadata": {},
   "source": [
    "### Vector index storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e248fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in a Simple Vector index - Day 2 Task 3\n",
    "import numpy as np\n",
    "\n",
    "vectors = np.array(chunk_embeddings)\n",
    "# Keep an array or list of chunk texts in the same order\n",
    "chunks_list = chunks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59f54cd",
   "metadata": {},
   "source": [
    "### Test the retreival with a query ⭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d83306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the retrieval on a Simple Query - Day 2 Task 4\n",
    "def retrieve(query, vectors, chunks_list, model):\n",
    "    '''Retrieve the most relevant chunk based on cosinle similarity'''\n",
    "    q_vec = model.encode([query])[0]\n",
    "    # Compute cosine similarty between q_vec and all chunk vectors\n",
    "    scores = np.dot(vectors, q_vec) / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(q_vec) + 1e-9)\n",
    "    top_indx = int(np.argmax(scores))\n",
    "    return chunks_list[top_indx], scores[top_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7e8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the result - Day 2 Task 5\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "Query = \"What is a cat lover called\"\n",
    "retrieve(query=Query, vectors=vectors, chunks_list=chunks_list, model=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9709ee15",
   "metadata": {},
   "source": [
    "### Save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your work - Day 2 Task 6\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Save \n",
    "np.save('embeddings.npy', vectors) # chunk_embeddings\n",
    "\n",
    "\n",
    "# save the chunk texts\n",
    "with open(\"chunks.json\", \"w\") as f:\n",
    "    json.dump(chunks_list, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b2008",
   "metadata": {},
   "source": [
    "### Load the embeddings ⭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbfd54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings - Day 2 Task 7\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "vectors = np.load(\"./data/embeddings.npy\")\n",
    "\n",
    "with open('./data/chunks.json', \"r\") as f:\n",
    "    chunks_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectors[:10])\n",
    "print(\"\\n\")\n",
    "print(chunks_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3584c26e",
   "metadata": {},
   "source": [
    "## Day 3: Building Your First RAG System (End-to-End QA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f5a4a",
   "metadata": {},
   "source": [
    "### Generate an Answer(Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c83f6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate Retrieval and Generation (PIPELINE VERSION) - Day 3 Task 1\n",
    "\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Load the model and tokenizer for generation (this might download weights the first time)\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "def answer_query(query, top_k=3):\n",
    "    # Retrieve top k chunks\n",
    "    q_vec = embedding_model.encode([query])[0] # embed the query using same model as before\n",
    "    scores = np.dot(vectors, q_vec) / (np.linalg.norm(vectors, axis=1)*np.linalg.norm(q_vec) + 1e-9)\n",
    "    top_indices = scores.argsort()[-top_k:][::-1] # indices of top k chunks, sorted by score desc\n",
    "    retrieved_chunks = [chunks_list[i] for i in top_indices] \n",
    "    # construct context string\n",
    "    context = \" \".join(retrieved_chunks)\n",
    "    prompt = (f\"Answer the question using ONLY the context below and Explain in detail. If the answer is not in the context, say 'I do not know.'\\n\\n\"\n",
    "              f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\")\n",
    "    result = generator(prompt, max_length=200, num_return_sequences=1)\n",
    "    answer = result[0]['generated_text']\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef261349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with known Question - Day 3 Task 2\n",
    "answer_query(\"What is the name of heaviest cat ever?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd473a",
   "metadata": {},
   "source": [
    "### Generate an Answer (AutomodelForSeq2LM) ⭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eadf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate Retrieval and Generation (AutoModelForSeq2SeqLM VERSION) - Day 3 Task 1\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Same with generator pipeline\n",
    "def generate_answer(prompt):\n",
    "    \"\"\"Generate  Answer using FLAN-T5\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(**inputs, max_length=500)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with known Question - Day 3 Task 2\n",
    "def answer_query(query):\n",
    "    context = retrieve(query, vectors, chunks_list, embedding_model)\n",
    "    # Refine prompt if needed - Day 3 Task 3\n",
    "    prompt =  (f\"\"\"\n",
    "                You are a QA assistant.\n",
    "\n",
    "                Rules:\n",
    "                - Use the context as the ONLY source of factual information.\n",
    "                - You may paraphrase and combine details into your own sentences.\n",
    "                - Do NOT add new facts that are not supported by the context.\n",
    "                - If the context does not contain the answer, say exactly: \"I do not know.\"\n",
    "\n",
    "                Task:\n",
    "                Answer the question in your own words.\n",
    "\n",
    "                Context:\n",
    "                {context}\n",
    "\n",
    "                Question: {query}\n",
    "\n",
    "                Answer:\"\"\") \n",
    "    # Logging - Day 3 Task 4\n",
    "    print(f\"Context: {context}\")\n",
    "    answer = generate_answer(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79edbdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_query(\"Lightiest cat ever?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81cf414",
   "metadata": {},
   "source": [
    "## Day 4: Building an Interactive RAG Application (UI Integration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59a71b",
   "metadata": {},
   "source": [
    "### Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d131c6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day 4 Task 1-2-3-4-5\n",
    "import gradio as gr\n",
    "\n",
    "def rag_system(query):\n",
    "    # Use our answer_query function from Day 3\n",
    "    answer = answer_query(query)\n",
    "    return answer\n",
    "\n",
    "iface = gr.Interface(fn=rag_system, inputs=\"text\", outputs=\"text\", title=\"RAG QA System\", description=\"Ask a question and get an answer from documents.\")\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f282f50",
   "metadata": {},
   "source": [
    "## Day 5: Adding Conversational Memory to RAG Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae677a",
   "metadata": {},
   "source": [
    "#### Necessary Fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "846a39c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caner\\Desktop\\RAG-Bootcamp\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load vectors\n",
    "import numpy as np\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Saved Vectors\n",
    "vectors = np.load(\"./data/embeddings.npy\")\n",
    "# Embedding model \n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# Saved Chunks\n",
    "with open('./data/chunks.json', \"r\") as f:\n",
    "    chunks_list = json.load(f)\n",
    "# Retrieval function\n",
    "def retrieve(query, vectors, chunks_list, model):\n",
    "    '''Retrieve the most relevant chunk based on cosinle similarity'''\n",
    "    q_vec = model.encode([query])[0]\n",
    "    # Compute cosine similarty between q_vec and all chunk vectors\n",
    "    scores = np.dot(vectors, q_vec) / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(q_vec) + 1e-9)\n",
    "    top_indx = int(np.argmax(scores))\n",
    "    return chunks_list[top_indx], scores[top_indx]\n",
    "\n",
    "# LLM model name to generate answer\n",
    "model_name = \"google/flan-t5-base\"\n",
    "# model library from HuggingFace\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "# Model tokenizer from HuggingFace\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# generate answer fucntion - same with generator pipeline\n",
    "def generate_answer(prompt):\n",
    "    \"\"\"Generate  Answer using FLAN-T5\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(**inputs, max_length=500)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526bc8fe",
   "metadata": {},
   "source": [
    "### Gradio UI (with history ⭐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a5584bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_SYSTEM_PROMPT =(\n",
    "    \"Answer the user's query using ONLY the CONTEXT and CHAT HISTORY below\"\n",
    "    \"Use CHAT HISTORY to resolve references like (e.g., it , they , them etc.)\"\n",
    "    \"If the answer is not in the context or CHAT HISTORY, say 'I do not know'.\"\n",
    ")\n",
    "\n",
    "def build_prompt(context, history, question, max_turns):\n",
    "    # keep last N turns to avoid prompt bloat\n",
    "    recent = history[-max_turns:] if history else []\n",
    "\n",
    "    history_block = \"\"\n",
    "    for q,a in recent:\n",
    "        history_block += f\"User {q}\\n Asisstant: {a}\\n\"\n",
    "\n",
    "    return (\n",
    "        f\"{NEW_SYSTEM_PROMPT}\\n\\n\"\n",
    "        f\"[CONTEXT]\\n{context}\\n\\n\"\n",
    "        f\"[CHAT HISTORY]\\n{history_block if history_block else '(none)'}\\n\\n\"\n",
    "        f\"[CURRENT QUESTION]\\nUser: {question}\\n Assistant:\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9999de42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import re\n",
    "\n",
    "LAST_DEBUG = {\"prompt\": \"\", \"retrieval_query\": \"\"}\n",
    "\n",
    "def make_retrieval_query(question: str, history: list[tuple[str, str]]) -> str:\n",
    "    if not history:\n",
    "        return question\n",
    "    last_q, _ = history[-1]\n",
    "    return f\"{last_q}\\nFollow-up: {question}\"\n",
    "\n",
    "def answer_query_with_history(question, history):\n",
    "    retrieval_query = make_retrieval_query(question, history) \n",
    "    # retrieve the context\n",
    "    context = retrieve(query=question, vectors=vectors,chunks_list=chunks_list,model=embedding_model)\n",
    "    prompt = build_prompt(context=context, history=history, question=question, max_turns=3)\n",
    "\n",
    "    # store for UI debugging\n",
    "    LAST_DEBUG[\"prompt\"] = prompt\n",
    "    LAST_DEBUG[\"retrieval_query\"] = retrieval_query\n",
    "\n",
    "    out = generate_answer(prompt)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _content_to_text(content):\n",
    "    '''\n",
    "    Gradio 6+ uses OpenAI-style structured content blocks.\n",
    "    Older versions often use plain strings.\n",
    "    '''\n",
    "    if content is None:\n",
    "        return \"\"\n",
    "    if isinstance(content, str):\n",
    "        return content\n",
    "    if isinstance(content, list):\n",
    "        # list of blocks, e.g., [{\"type\":\"text\", \"text\":\"hi\"}]\n",
    "        parts = []\n",
    "        for block in content:\n",
    "            if isinstance(block, dict) and block.get(\"type\") == \"text\":\n",
    "                parts.append(block.get(\"text\", \"\"))\n",
    "            elif isinstance(block, str):\n",
    "                parts.append(block)\n",
    "        return \"\".join(parts)\n",
    "    if isinstance(content, dict) and \"text\" in content:\n",
    "        return content [\"text\"]\n",
    "    return str(content)\n",
    "\n",
    "\n",
    "def normalize_gradio_history(history):\n",
    "    \"\"\"\n",
    "    Supports:\n",
    "    - v4/v5 style: [[user,bot], ...]\n",
    "    - v6 messages style: [{\"role\":\"user\", \"content\":[...]}, {\"role\":\"assistant\",\"content\":[...]} , ...]\n",
    "    Returns: list[tuple[user_text, assistant_text]]\n",
    "    \"\"\"\n",
    "    if not history:\n",
    "        return []\n",
    "    \n",
    "    # Pair format\n",
    "    if isinstance(history, list) and history and isinstance(history[0], (list, tuple)) and len(history[0]) == 2:\n",
    "        out = []\n",
    "        for u, a in history:\n",
    "            out.append((_content_to_text(u), _content_to_text(a)))\n",
    "        return out\n",
    "    \n",
    "    # Messages format\n",
    "    if isinstance(history, list) and history and isinstance(history[0], dict) and \"role\" in history[0]:\n",
    "        pairs = []\n",
    "        pending_user = None\n",
    "        for msg in history:\n",
    "            role = msg.get(\"role\")\n",
    "            text = _content_to_text(msg.get(\"content\"))\n",
    "            if role == \"user\":\n",
    "                pending_user = text\n",
    "            elif role == \"assistant\" and pending_user is not None:\n",
    "                pairs.append((pending_user, text))\n",
    "                pending_user = None\n",
    "        return pairs\n",
    "    \n",
    "    # Fallback\n",
    "    return []\n",
    "\n",
    "def chatbot_fn(message, history):\n",
    "    history_pairs = normalize_gradio_history(history)\n",
    "\n",
    "    answer = answer_query_with_history(message, history_pairs)\n",
    "\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961a001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.ChatInterface(\n",
    "    fn=chatbot_fn,\n",
    "    title=\"Day 5 RAG + Chat History Test\",\n",
    ")\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2026dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Day 5 RAG + Chat History (with Debug)\")\n",
    "    chat = gr.Chatbot()\n",
    "    msg = gr.Textbox(placeholder=\"Ask something…\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    with gr.Accordion(\"Debug (what the model sees)\", open=False):\n",
    "        dbg_retrieval = gr.Textbox(label=\"Retrieval Query\", lines=2)\n",
    "        dbg_prompt = gr.Textbox(label=\"Final Prompt\", lines=18)\n",
    "\n",
    "    def respond(message, history):\n",
    "        # history is a list of {\"role\": ..., \"content\": ...} dicts in messages mode\n",
    "        history_pairs = normalize_gradio_history(history)  # your helper: -> list[(user, assistant)]\n",
    "        answer = answer_query_with_history(message, history_pairs)\n",
    "\n",
    "        history = history or []\n",
    "        history = history + [\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "            {\"role\": \"assistant\", \"content\": answer},\n",
    "        ]\n",
    "\n",
    "        return history, \"\", LAST_DEBUG.get(\"retrieval_query\",\"\"), LAST_DEBUG.get(\"prompt\",\"\")\n",
    "\n",
    "    msg.submit(respond, [msg, chat], [chat, msg, dbg_retrieval, dbg_prompt])\n",
    "    clear.click(lambda: ([], \"\", \"\", \"\"), None, [chat, msg, dbg_retrieval, dbg_prompt])\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e13099",
   "metadata": {},
   "source": [
    "## Day 6 - Deploying the RAG system (Local to Cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e0720b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
