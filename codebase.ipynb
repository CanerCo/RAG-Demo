{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e86307d",
   "metadata": {},
   "source": [
    "## Day 1 - Introduction to RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b2d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Environment - Day 1 Task 1\n",
    "%pip install transformers sentence-transformers  faiss-cpu gradio streamlit chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dfcdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a Dataset - Day 1 Task 2 - 3 - 4\n",
    "## Dataset: ./data/cat-facts.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7be255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caner\\Desktop\\RAG-Bootcamp\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\caner\\Desktop\\RAG-Bootcamp\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\caner\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=10) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"Hello, my name is Sam. I am here to inform you that you need to join the Brotherhood. I have a new weapon... and you need to join us! You need to join us! We will defeat you! In order to do that, we need you and your weapon. So, go to the Brotherhood Chamber and tell them all to save us. I will be the one to stop you! We will defeat you! Our goal is simple: save humanity from a future of destruction. And you... you know, I know you're not alone. You know that the Brotherhood has become more dangerous due to fear! I don't want to be the only one to be killed. I want to save humanity from a future where we all die. But even if you've never heard of me before, this is the first time I'm speaking. I wanted to say that we've met in an elevator that I've never seen before. There is only one way to the elevator. The first time I saw you, I thought you were a hero, and I wanted to meet you. I wanted to tell you how I lost my life to save you from the Brotherhood. But that was too late. I had to get to the elevator. So I decided to join the Brotherhood. To save\"}]\n"
     ]
    }
   ],
   "source": [
    "# Test Environment - Day 1 Task 5\n",
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "print(generator(\"Hello, my name is\", max_length=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58667a8",
   "metadata": {},
   "source": [
    "## Day 2 - Core Components of a RAG Pipeline (Data, Embeddings, and Retrieval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a99da3",
   "metadata": {},
   "source": [
    "### Chunking implementation (On Cat Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb70b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunking implementation - Day 2 Task 1\n",
    "def chunk_text(text, max_length=500):\n",
    "    # Text is splitted into chunks at most max_length characters, at sentence boundaries if possible\n",
    "    import re\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text.strip()) # split on sentence end\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) + 1  <= max_length:\n",
    "            current_chunk += sentence + \" \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \" \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cdb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On average, cats spend 2/3 of every day sleeping. That means a nine-year-old cat has been awake for only three years of its life. Unlike dogs, cats do not have a sweet tooth. Scientists believe this is due to a mutation in a key taste receptor. When a cat chases its prey, it keeps its head level. Dogs and humans bob their heads up and down.\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/cat-facts.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# First Chunk\n",
    "print(chunk_text(text=text, max_length=500)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d3f415",
   "metadata": {},
   "source": [
    "### Embedding the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04186f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding the Chunks - Day 2  Task 2\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "chunks = chunk_text(text, max_length=500)\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "chunk_embeddings = embedding_model.encode(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0c6d0d",
   "metadata": {},
   "source": [
    "### Vector index storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e248fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in a Simple Vector index - Day 2 Task 3\n",
    "import numpy as np\n",
    "\n",
    "vectors = np.array(chunk_embeddings)\n",
    "# Keep an array or list of chunk texts in the same order\n",
    "chunks_list = chunks \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59f54cd",
   "metadata": {},
   "source": [
    "### Test the retreival with a query ⭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0d83306",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the retrieval on a Simple Query - Day 2 Task 4\n",
    "def retrieve(query, vectors, chunks_list, model):\n",
    "    '''Retrieve the most relevant chunk based on cosinle similarity'''\n",
    "    q_vec = model.encode([query])[0]\n",
    "    # Compute cosine similarty between q_vec and all chunk vectors\n",
    "    scores = np.dot(vectors, q_vec) / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(q_vec) + 1e-9)\n",
    "    top_indx = int(np.argmax(scores))\n",
    "    return chunks_list[top_indx], scores[top_indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea7e8ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Two members of the cat family are distinct from all others: the clouded leopard and the cheetah. The clouded leopard does not roar like other big cats, nor does it groom or rest like small cats. The cheetah is unique because it is a running cat; all others are leaping cats. They are leaping cats because they slowly stalk their prey and then leap on it. A cat lover is called an Ailurophilia (Greek: cat+lover). In Japan, cats are thought to have the power to turn into super spirits when they die.',\n",
       " np.float32(0.5999281))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the result - Day 2 Task 5\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "Query = \"What is a cat lover called\"\n",
    "retrieve(query=Query, vectors=vectors, chunks_list=chunks_list, model=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9709ee15",
   "metadata": {},
   "source": [
    "### Save embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9b917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your work - Day 2 Task 6\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Save \n",
    "np.save('embeddings.npy', vectors) # chunk_embeddings\n",
    "\n",
    "\n",
    "# save the chunk texts\n",
    "with open(\"chunks.json\", \"w\") as f:\n",
    "    json.dump(chunks_list, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b2008",
   "metadata": {},
   "source": [
    "### Load the embeddings ⭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbfd54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embeddings - Day 2 Task 7\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "vectors = np.load(\"./data/embeddings.npy\")\n",
    "\n",
    "with open('./data/chunks.json', \"r\") as f:\n",
    "    chunks_list = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90f3b5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.09816721 -0.06158825  0.04418764 ...  0.04759893  0.00374702\n",
      "  -0.02113941]\n",
      " [ 0.08874442 -0.03300164  0.06386363 ...  0.09614801  0.06132019\n",
      "   0.08886918]\n",
      " [ 0.1137786   0.02467443  0.04589037 ...  0.11650186  0.06650402\n",
      "   0.02615136]\n",
      " ...\n",
      " [ 0.14878643 -0.05136343  0.05026303 ...  0.05624724 -0.01553893\n",
      "   0.10549022]\n",
      " [ 0.07461078 -0.09752137  0.0282176  ...  0.04340719  0.04313685\n",
      "   0.03407015]\n",
      " [ 0.04919337  0.08810407  0.01938833 ...  0.07192653  0.06094994\n",
      "  -0.00545023]]\n",
      "\n",
      "\n",
      "['On average, cats spend 2/3 of every day sleeping. That means a nine-year-old cat has been awake for only three years of its life. Unlike dogs, cats do not have a sweet tooth. Scientists believe this is due to a mutation in a key taste receptor. When a cat chases its prey, it keeps its head level. Dogs and humans bob their heads up and down.', 'The technical term for a cat’s hairball is a “bezoar.”\\nA group of cats is called a “clowder.”\\nFemale cats tend to be right pawed, while male cats are more often left pawed. Interestingly, while 90% of humans are right handed, the remaining 10% of lefties also tend to be male. A cat can’t climb head first down a tree because every claw on a cat’s paw points the same way. To get down from a tree, a cat must back down. Cats make about 100 different sounds. Dogs make only about 10.', 'A cat’s brain is biologically more similar to a human brain than it is to a dog’s. Both humans and cats have identical regions in their brains that are responsible for emotions. There are more than 500 million domestic cats in the world, with approximately 40 recognized breeds. While it is commonly thought that the ancient Egyptians were the first to domesticate cats, the oldest known pet cat was recently found in a 9,500-year-old grave on the Mediterranean island of Cyprus.', 'This grave predates early Egyptian art depicting cats by 4,000 years or more. During the time of the Spanish Inquisition, Pope Innocent VIII condemned cats as evil and thousands of cats were burned. Unfortunately, the widespread killing of cats led to an explosion of the rat population, which exacerbated the effects of the Black Death. During the Middle Ages, cats were associated with witchcraft, and on St.', 'John’s Day, people all over Europe would stuff them into sacks and toss the cats into bonfires. On holy days, people celebrated by tossing cats from church towers. Cats are North America’s most popular pets: there are 73 million cats compared to 63 million dogs. Over 30% of households in North America own a cat. The first cat in space was a French cat named Felicette (a.k.a. “Astrocat”) In 1963, France blasted the cat into outer space.', 'Electrodes implanted in her brains sent neurological signals back to Earth. She survived the trip. The group of words associated with cat ( catt, cath, chat, katze ) stem from the Latin catus , meaning domestic cat, as opposed to feles , or wild cat. The term “puss” is the root of the principal word for “cat” in the Romanian term pisica and the root of secondary words in Lithuanian ( puz ) and Low German puus .', 'Some scholars suggest that “puss” could be imitative of the hissing sound used to get a cat’s attention. As a slang word for the female pudenda, it could be associated with the connotation of a cat being soft, warm, and fuzzy. Approximately 40,000 people are bitten by cats in the U.S. annually. According to Hebrew legend, Noah prayed to God for help protecting all the food he stored on the ark from being eaten by rats. In reply, God made the lion sneeze, and out popped a cat.', 'A cat’s hearing is better than a dog’s. And a cat can hear high-frequency sounds up to two octaves higher than a human. A cat can travel at a top speed of approximately 31 mph (49 km) over a short distance. A cat can jump up to five times its own height in a single bound. Some cats have survived falls of over 65 feet (20 meters), due largely to their “righting reflex.” The eyes and balance organs in the inner ear tell it where it is in space so the cat can land on its feet.', 'Even cats without a tail have this ability. A cat rubs against people not only to be affectionate but also to mark out its territory with scent glands around its face. The tail area and paws also carry the cat’s scent. Researchers are unsure exactly how a cat purrs. Most veterinarians believe that a cat purrs by vibrating vocal folds deep in the throat. To do this, a muscle in the larynx opens and closes the air passage about 25 times per second.', 'When a family cat died in ancient Egypt, family members would mourn by shaving off their eyebrows. They also held elaborate funerals during which they drank wine and beat their breasts. The cat was embalmed with a sculpted wooden mask and the tiny mummy was placed in the family tomb or in a pet cemetery with tiny mummies of mice. In 1888, more than 300,000 mummified cats were found an Egyptian cemetery.']\n"
     ]
    }
   ],
   "source": [
    "print(vectors[:10])\n",
    "print(\"\\n\")\n",
    "print(chunks_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3584c26e",
   "metadata": {},
   "source": [
    "## Day 3: Building Your First RAG System (End-to-End QA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f5a4a",
   "metadata": {},
   "source": [
    "### Generate an Answer(Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c83f6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Integrate Retrieval and Generation (PIPELINE VERSION) - Day 3 Task 1\n",
    "\n",
    "from transformers import pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Load the model and tokenizer for generation (this might download weights the first time)\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "\n",
    "def answer_query(query, top_k=3):\n",
    "    # Retrieve top k chunks\n",
    "    q_vec = embedding_model.encode([query])[0] # embed the query using same model as before\n",
    "    scores = np.dot(vectors, q_vec) / (np.linalg.norm(vectors, axis=1)*np.linalg.norm(q_vec) + 1e-9)\n",
    "    top_indices = scores.argsort()[-top_k:][::-1] # indices of top k chunks, sorted by score desc\n",
    "    retrieved_chunks = [chunks_list[i] for i in top_indices] \n",
    "    # construct context string\n",
    "    context = \" \".join(retrieved_chunks)\n",
    "    prompt = (f\"Answer the question using ONLY the context below and Explain in detail. If the answer is not in the context, say 'I do not know.'\\n\\n\"\n",
    "              f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\")\n",
    "    result = generator(prompt, max_length=200, num_return_sequences=1)\n",
    "    answer = result[0]['generated_text']\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef261349",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=256) and `max_length`(=200) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Himmy'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with known Question - Day 3 Task 2\n",
    "answer_query(\"What is the name of heaviest cat ever?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd473a",
   "metadata": {},
   "source": [
    "### Generate an Answer (AutomodelForSeq2LM) ⭐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44eadf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate Retrieval and Generation (AutoModelForSeq2SeqLM VERSION) - Day 3 Task 1\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# Same with generator pipeline\n",
    "def generate_answer(prompt):\n",
    "    \"\"\"Generate  Answer using FLAN-T5\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(**inputs, max_length=500)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842c93ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with known Question - Day 3 Task 2\n",
    "def answer_query(query):\n",
    "    context = retrieve(query, vectors, chunks_list, embedding_model)\n",
    "    # Refine prompt if needed - Day 3 Task 3\n",
    "    prompt =  (f\"\"\"\n",
    "                You are a QA assistant.\n",
    "\n",
    "                Rules:\n",
    "                - Use the context as the ONLY source of factual information.\n",
    "                - You may paraphrase and combine details into your own sentences.\n",
    "                - Do NOT add new facts that are not supported by the context.\n",
    "                - If the context does not contain the answer, say exactly: \"I do not know.\"\n",
    "\n",
    "                Task:\n",
    "                Answer the question in your own words.\n",
    "\n",
    "                Context:\n",
    "                {context}\n",
    "\n",
    "                Question: {query}\n",
    "\n",
    "                Answer:\"\"\") \n",
    "    # Logging - Day 3 Task 4\n",
    "    print(f\"Context: {context}\")\n",
    "    answer = generate_answer(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79edbdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blue point Himalayan called Tinker Toy.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_query(\"Lightiest cat ever?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81cf414",
   "metadata": {},
   "source": [
    "## Day 4: Building an Interactive RAG Application (UI Integration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59a71b",
   "metadata": {},
   "source": [
    "### Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d131c6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Day 4 Task 1-2-3-4-5\n",
    "import gradio as gr\n",
    "\n",
    "def rag_system(query):\n",
    "    # Use our answer_query function from Day 3\n",
    "    answer = answer_query(query)\n",
    "    return answer\n",
    "\n",
    "iface = gr.Interface(fn=rag_system, inputs=\"text\", outputs=\"text\", title=\"RAG QA System\", description=\"Ask a question and get an answer from documents.\")\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f282f50",
   "metadata": {},
   "source": [
    "## Day 5: Adding Conversational Memory to RAG Assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae677a",
   "metadata": {},
   "source": [
    "#### Necessary Fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a39c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vectors\n",
    "import numpy as np\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Saved Vectors\n",
    "vectors = np.load(\"./data/embeddings.npy\")\n",
    "# Embedding model \n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# Saved Chunks\n",
    "with open('./data/chunks.json', \"r\") as f:\n",
    "    chunks_list = json.load(f)\n",
    "# Retrieval function\n",
    "def retrieve(query, vectors, chunks_list, model):\n",
    "    '''Retrieve the most relevant chunk based on cosinle similarity'''\n",
    "    q_vec = model.encode([query])[0]\n",
    "    # Compute cosine similarty between q_vec and all chunk vectors\n",
    "    scores = np.dot(vectors, q_vec) / (np.linalg.norm(vectors, axis=1) * np.linalg.norm(q_vec) + 1e-9)\n",
    "    top_indx = int(np.argmax(scores))\n",
    "    return chunks_list[top_indx], scores[top_indx]\n",
    "\n",
    "# LLM model name to generate answer\n",
    "model_name = \"google/flan-t5-base\"\n",
    "# model library from HuggingFace\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "# Model tokenizer from HuggingFace\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# generate answer fucntion - same with generator pipeline\n",
    "def generate_answer(prompt):\n",
    "    \"\"\"Generate  Answer using FLAN-T5\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt')\n",
    "    outputs = model.generate(**inputs, max_length=500)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526bc8fe",
   "metadata": {},
   "source": [
    "### Gradio UI (with history ⭐)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5584bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
